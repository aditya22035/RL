{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions:  4\n",
      "Number of states:  48\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CliffWalking-v0')\n",
    "numactions = env.action_space.n\n",
    "numstates = env.observation_space.n\n",
    "print(\"Number of actions: \", numactions)\n",
    "print(\"Number of states: \", numstates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(epsilon,Q,env):\n",
    "  def policy(state):\n",
    "    if state not in Q:\n",
    "      #take any action with equal probability\n",
    "      return np.random.choice(env.action_space.n)\n",
    "    else:\n",
    "      #take the best action with probability 1-epsilon\n",
    "      if np.random.random() > epsilon:\n",
    "        return np.argmax(Q[state])\n",
    "      else:\n",
    "        return np.random.choice(env.action_space.n)\n",
    "  return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_nstep(env,numeps,epsilon,alpha,gamma,nstep):\n",
    "  Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "  \n",
    "  for i in range(numeps):\n",
    "    if i % 10000 == 0:\n",
    "      print(\"Episode: \", i)\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    T = float('inf')\n",
    "    t = 0\n",
    "    #initalise the policy\n",
    "    policy = epsilon_greedy_policy(epsilon,Q,env)\n",
    "    \n",
    "    #stores state,action,reward tuples\n",
    "    buffer = []\n",
    "    \n",
    "    #initialise the first action\n",
    "    action = policy(state)\n",
    "    \n",
    "    while not done:\n",
    "      if t < T:\n",
    "        tup = env.step(action) \n",
    "        next_state, reward, terminated, truncated = tup[0], tup[1], tup[2], tup[3]\n",
    "        done = terminated or truncated\n",
    "        buffer.append((state,action,reward))\n",
    "        if done:\n",
    "          T = t+1\n",
    "        else:\n",
    "          state = next_state\n",
    "          action = policy(state)\n",
    "          \n",
    "      tau = t-nstep+1\n",
    "      \n",
    "      if tau >= 0:\n",
    "        G = 0\n",
    "        j = min(tau+nstep,T)\n",
    "\n",
    "        for k in range(tau+1, j+1):\n",
    "          G += (gamma**(k-tau-1))*buffer[k-1][2]\n",
    "\n",
    "        if tau+nstep < T:\n",
    "          state_tau_n = buffer[tau+nstep-1][0]\n",
    "          action_tau_n = buffer[tau+nstep-1][1]\n",
    "          G = G + (gamma**nstep)*Q[state_tau_n][action_tau_n]\n",
    "\n",
    "        state_tau = buffer[tau][0]\n",
    "        action_tau = buffer[tau][1]\n",
    "        Q[state_tau][action_tau] += alpha*(G-Q[state_tau][action_tau])        \n",
    "      t += 1\n",
    "      \n",
    "      if tau == T-1:\n",
    "        break\n",
    "          \n",
    "  return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeps = 10000\n",
    "epsilon = 0.1\n",
    "alpha = 0.1\n",
    "nsteps = 5\n",
    "gamma = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0\n"
     ]
    }
   ],
   "source": [
    "Q, policy = sarsa_nstep(env,numeps,epsilon,alpha,gamma,nsteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[564], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Plot the optimal path\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[43mplot_cliffwalking_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[564], line 23\u001b[0m, in \u001b[0;36mplot_cliffwalking_paths\u001b[0;34m(Q, env)\u001b[0m\n\u001b[1;32m     21\u001b[0m     optimal_path\u001b[38;5;241m.\u001b[39mappend((row, col))\n\u001b[1;32m     22\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(Q[state]) \n\u001b[0;32m---> 23\u001b[0m     tup \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     state, _, done, _ \u001b[38;5;241m=\u001b[39m tup \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tup) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (tup[\u001b[38;5;241m0\u001b[39m], tup[\u001b[38;5;241m1\u001b[39m], tup[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Mark the start and goal positions\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/common.py:277\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_render: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose_called: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    280\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment that on the first call will run the `passive_env_step_check`.\"\"\"\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def plot_cliffwalking_paths(Q, env):\n",
    "    grid_rows, grid_cols = 4, 12  # Grid dimensions for CliffWalking-v0\n",
    "    start_state = 36  # Start state index\n",
    "    goal_state = 47   # Goal state index\n",
    "\n",
    "    # Initialize the grid\n",
    "    grid = np.zeros((grid_rows, grid_cols), dtype=int)\n",
    "\n",
    "    # Define the cliff area\n",
    "    cliff_indices = np.arange(37, 47)\n",
    "    for idx in cliff_indices:\n",
    "        row, col = divmod(idx, grid_cols)\n",
    "        grid[row, col] = -100  # Cliff cells\n",
    "\n",
    "    # Compute the optimal path\n",
    "    state, _ = env.reset()\n",
    "    optimal_path = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        row, col = divmod(state, grid_cols)\n",
    "        optimal_path.append((row, col))\n",
    "        action = np.argmax(Q[state]) \n",
    "        tup = env.step(action)\n",
    "        state, _, done, _ = tup if len(tup) == 4 else (tup[0], tup[1], tup[2], None)\n",
    "\n",
    "    # Mark the start and goal positions\n",
    "    start_row, start_col = divmod(start_state, grid_cols)\n",
    "    goal_row, goal_col = divmod(goal_state, grid_cols)\n",
    "\n",
    "    # Plot the grid\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    for r in range(grid_rows):\n",
    "        for c in range(grid_cols):\n",
    "            if (r, c) in optimal_path:\n",
    "                color = \"red\" if (r, c) != (start_row, start_col) and (r, c) != (goal_row, goal_col) else \"green\"\n",
    "                plt.text(c, r, 'O', ha='center', va='center', color=color, fontsize=12, fontweight='bold')\n",
    "            elif grid[r, c] == -100:\n",
    "                plt.text(c, r, 'Cliff', ha='center', va='center', color='gray', fontsize=8, fontweight='bold')\n",
    "\n",
    "    # Highlight start and goal\n",
    "    plt.text(start_col, start_row, 'S', ha='center', va='center', color='blue', fontsize=14, fontweight='bold')\n",
    "    plt.text(goal_col, goal_row, 'G', ha='center', va='center', color='blue', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Draw the grid\n",
    "    plt.xlim(-0.5, grid_cols - 0.5)\n",
    "    plt.ylim(grid_rows - 0.5, -0.5)\n",
    "    plt.xticks(range(grid_cols))\n",
    "    plt.yticks(range(grid_rows))\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Cliff Walking Optimal Path\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot the optimal path\n",
    "plot_cliffwalking_paths(Q, env)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
