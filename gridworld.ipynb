{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "  def __init__(self) -> None:\n",
    "    self.size = 5\n",
    "    self.start = (0, 0)\n",
    "    self.gamma = 0.9\n",
    "    self.actions = [(-1,0), (0,1), (1,0), (0,-1)]#up,right,down,left\n",
    "    self.V = np.zeros((self.size, self.size))\n",
    "    self.pi = np.ones((self.size, self.size, len(self.actions))) / len(self.actions) #uniform policy\n",
    "\n",
    "  def is_valid_state(self, state):\n",
    "    return (0 <= state[0] < self.size and \n",
    "            0 <= state[1] < self.size)\n",
    "    \n",
    "  # def get_reward_and_next_state(self, state):\n",
    "  #   i, j = state\n",
    "  #   if i == 0 and j == 1:\n",
    "  #     return 10, (4, 1)\n",
    "  #   elif i == 0 and j == 3:\n",
    "  #     return 5, (2, 3)\n",
    "  #   return 0, state\n",
    "      \n",
    "  def policy_eval(self):\n",
    "    theta = 0.001\n",
    "    while True:\n",
    "      delta = 0\n",
    "      for i in range(self.size):\n",
    "        for j in range(self.size):\n",
    "          current_state = (i, j)\n",
    "          Vold = self.V[i, j]\n",
    "          \n",
    "          if i == 0 and j == 1:\n",
    "            self.V[i, j] = 10 + self.gamma * self.V[4, 1]\n",
    "          elif i == 0 and j == 3:\n",
    "            self.V[i, j] = 5 + self.gamma * self.V[2, 3]\n",
    "          else:\n",
    "            value = 0\n",
    "            for idx, action in enumerate(self.actions):\n",
    "              next_state = (i + action[0], j + action[1])\n",
    "              if self.is_valid_state(next_state):\n",
    "                value += self.pi[i, j, idx] * (0 + self.gamma * self.V[next_state])\n",
    "              else:\n",
    "                value += self.pi[i, j, idx] * (-1 + self.gamma * self.V[current_state])\n",
    "            self.V[i, j] = value\n",
    "          delta = max(delta, abs(Vold - self.V[i, j]))\n",
    "      if delta < theta:\n",
    "        break\n",
    "  \n",
    "  def policy_improvement(self):\n",
    "    policy_stable = True\n",
    "    for i in range(self.size):\n",
    "      for j in range(self.size):\n",
    "        current_state = (i, j)\n",
    "        old_action = self.pi[i, j].copy()\n",
    "        Q = np.zeros(len(self.actions))\n",
    "        if(i == 0 and j == 1):\n",
    "          Q[0] = 10 + self.gamma * self.V[4][1]\n",
    "        elif i == 0 and j == 3:\n",
    "          Q[2] = 5 + self.gamma * self.V[2][3]\n",
    "        else:\n",
    "          for idx, action in enumerate(self.actions):\n",
    "            next_state = (i + action[0], j + action[1])\n",
    "            if next_state[0] >= 0 and next_state[0] < self.size and next_state[1] >= 0 and next_state[1] < self.size:\n",
    "              Q[idx] = 0 + self.gamma * self.V[next_state]\n",
    "            else:\n",
    "              Q[idx] = -1 + self.gamma * self.V[current_state]\n",
    "        new_best_action = np.argmax(Q)\n",
    "        new_action = np.zeros(len(self.actions))\n",
    "        new_action[new_best_action] = 1\n",
    "        self.pi[i][j] = new_action\n",
    "        if not np.array_equal(old_action, new_action):\n",
    "          policy_stable = False\n",
    "\n",
    "    return policy_stable\n",
    "  \n",
    "  def policy_iteration(self):\n",
    "    policy_stable = False\n",
    "    while not policy_stable:\n",
    "      self.policy_eval()\n",
    "      policy_stable = self.policy_improvement()\n",
    "    return self.V, self.pi\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21.97748462 24.41942766 21.97748489 19.41942766 17.47748489]\n",
      " [19.77973615 21.97748489 19.7797364  17.80176276 16.02158649]\n",
      " [17.80176254 19.7797364  17.80176276 16.02158649 14.41942784]\n",
      " [16.02158629 17.80176276 16.02158649 14.41942784 12.97748505]\n",
      " [14.41942766 16.02158649 14.41942784 12.97748505 11.67973655]]\n"
     ]
    }
   ],
   "source": [
    "#running policy iteration\n",
    "grid = GridWorld()\n",
    "V, pi = grid.policy_iteration()\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "  def __init__(self) -> None:\n",
    "    self.size = 5\n",
    "    self.start = (0, 0)\n",
    "    self.gamma = 0.9\n",
    "    self.actions = [(-1,0), (0,1), (1,0), (0,-1)]#up,right,down,left\n",
    "    self.V = np.zeros((self.size, self.size))\n",
    "    self.pi = np.ones((self.size, self.size, len(self.actions))) / len(self.actions) #uniform policy\n",
    "  def pick_action(self):\n",
    "    return np.random.choice(self.actions)\n",
    "  \n",
    "      \n",
    "  def policy_eval(self):\n",
    "    theta = 0.001\n",
    "    while True:\n",
    "      delta = 0\n",
    "      for i in range(self.size):\n",
    "        for j in range(self.size):\n",
    "          current_state = (i, j)\n",
    "          reward, next_state = 0, (0,0)\n",
    "          Vold = self.V[current_state]\n",
    "          if i == 0 and j == 1:\n",
    "            reward = 10\n",
    "            next_state = (4,1)\n",
    "            self.V[current_state] = reward + self.gamma * self.V[next_state]\n",
    "          elif i == 0 and j == 3:\n",
    "            reward = 5\n",
    "            next_state = (2,3)\n",
    "            self.V[current_state] = reward + self.gamma * self.V[next_state]\n",
    "          else:\n",
    "            sum = 0\n",
    "            for idx, action in enumerate(self.actions):\n",
    "              next_state = (i + action[0], j + action[1])\n",
    "              if next_state[0] >= 0 and next_state[0] < self.size and next_state[1] >= 0 and next_state[1] < self.size:\n",
    "                sum += self.pi[i][j][idx] * (0 + self.gamma * self.V[next_state])  \n",
    "              else:\n",
    "                sum += self.pi[i][j][idx] * (-1 + self.gamma * self.V[current_state])\n",
    "            self.V[current_state] = sum\n",
    "          delta = max(delta, abs(Vold - self.V[current_state]))\n",
    "      if delta < theta:\n",
    "        break\n",
    "    return self.V\n",
    "  \n",
    "  def policy_improvement(self):\n",
    "    policy_stable = True\n",
    "    for i in range(self.size):\n",
    "      for j in range(self.size):\n",
    "        current_state = (i, j)\n",
    "        old_action = self.pi[i][j]\n",
    "        Q = np.zeros(len(self.actions))\n",
    "        if(i == 0 and j == 1):\n",
    "          Q[0] = 10 + self.gamma * self.V[4][1]\n",
    "        elif i == 0 and j == 3:\n",
    "          Q[2] = 5 + self.gamma * self.V[2][3]\n",
    "        else:\n",
    "          for idx, action in enumerate(self.actions):\n",
    "            next_state = (i + action[0], j + action[1])\n",
    "            if next_state[0] >= 0 and next_state[0] < self.size and next_state[1] >= 0 and next_state[1] < self.size:\n",
    "              Q[idx] = 0 + self.gamma * self.V[next_state]\n",
    "            else:\n",
    "              Q[idx] = -1 + self.gamma * self.V[current_state]\n",
    "        new_best_action = np.argmax(Q)\n",
    "        new_action = np.zeros(len(self.actions))\n",
    "        new_action[new_best_action] = 1\n",
    "        self.pi[i][j] = new_action\n",
    "        if not np.array_equal(old_action, new_action):\n",
    "          policy_stable = False\n",
    "\n",
    "    return policy_stable\n",
    "  \n",
    "  def policy_iteration(self):\n",
    "    policy_stable = False\n",
    "    while not policy_stable:\n",
    "      self.policy_eval()\n",
    "      policy_stable = self.policy_improvement()\n",
    "    return self.V, self.pi\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.31359559  8.79292942  4.43113177  5.32556099  1.4955287 ]\n",
      " [ 1.52582318  2.99591435  2.2534199   1.91064941  0.55045095]\n",
      " [ 0.05486787  0.74165922  0.67626363  0.36114423 -0.40025498]\n",
      " [-0.96965064 -0.43208514 -0.35180898 -0.58272448 -1.18027658]\n",
      " [-1.85380443 -1.34185832 -1.22622928 -1.42007309 -1.97241846]]\n"
     ]
    }
   ],
   "source": [
    "#running policy iteration\n",
    "grid = GridWorld()\n",
    "V, pi = grid.policy_iteration()\n",
    "print(V)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
